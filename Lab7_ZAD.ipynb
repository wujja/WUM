{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+uYnWCqnTV1YSQyZXmM9l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Treść zadania\n"],"metadata":{"id":"-JBe6rFXMcnj"}},{"cell_type":"code","source":["# cel: zbudowac siec neuronową, ktora bedzie klasyfikowala artykul prasowy do jednej z 46 kategorii tematycznych\n","# dane: Reuters data\n","# ustalic liczbe slow, do ktorych ograniczymy reprezentacje\n","# wgrac zbior danych z tensorflow.keras.datasets (reuters, i jego metoda load_data())\n","# sprawdzic ksztalty tensorow\n","# obejrzec dane\n","# czyli sa to listy list (identyfikatorow slow, podobnie jak w imdb)\n","# dokonac krotkiej eksploracyjnej analizy danych\n","# czy sa same liczby 0-size, (czy nie ma np. znakow '?')\n","# obejrzec formę etykiet (ytrain, ytest)\n","# obejrzec jakie wartosci przyjmuje ytrain, ytest (czyli ile jest kategorii i jakie)\n","# obejrzec rozklad klas w ytrain i ytest (czy klasy sa zrownowazone?, czy ytrain i ytest maja podobne rozklady?)\n","# przetwarzanie danych\n","# zakodujmy wektory X jako \"bag of words\" (funkcja vectorize, tak jak w przykladzie z danymi imdb)\n","# napisac funkcję, ktora zakoduje tez etykiety jako 1-hot-encoding  ##\n","# (lub skorzystac w tym celu z gotowej funkcji to_categorical z tensorflow.keras.utils)\n","# zbudowac siec (interfejs \"Sequential\") o 2 warstwach ukrytych po 64 neurony (ile powinna miec neuronow 3. warstwa (wyjsciowa)?)\n","# jakich funkcji aktywacji nalezy uzyc w ukrytych a jakiej w warstwie wyjsciowej?\n","# skompilowac model (compile). Jakiej funkcji straty uzyc? (mozna jako metryki jakosci uzyc accuracy, dla upr.)\n","# wziac pierwszych 1000 obserwacji jako walidacyjne (podzielic odpowiednio zbiory na val i part)\n","# wytrenuj model na czesciowym zb. treningowym (20 epok, batch = 128) \n","# przekazac dane walidacyjne do sledzenia historii uczenia, zeby dokonac early stopping\n","# uwaga: nie zapomniec, ze funkcja \"fit\" zwraca historię uczenia (zapamietac to w zmiennej)\n","# przeanalizuj historię uczenia (loss/accuracy na czesc.treningowym i walidacyjnym)\n","# wykonaj wykresy tych wielkosci jako funkcja epoki\n","# jaka liczba epok wyglada na optymalną?\n","# wytrenuj teraz model od poczatku na calym treningowym dla takiej liczby epok\n","# uwaga: nalezy od nowa zbudowac model, skompilowac go i wytrenowac \n","#(samo wywolanie fit bedzie tylko kontynuowac uczenie na (przetrenowanym) starym modelu)\n","# dokonac ewaluacji na testowym\n","***\n","# dalsze eksperymentowanie:\n","# 1) eksperymentuj z kilkoma (min. 2) wariantami modelu (zmieniajac glebokosc, szerokosc warstw)\n","# np. co sie stanie jak zmniejszymy szerokosc 2. warstwy ukrytej? (tzw. bottleneck)\n","# 2) porownaj jak sobie poradzi jakis klasyczny klasyfikator na tych danych (ze scikit-learn)\n","\n","\n","\n","##funkcja straty dla wielu klas spars entropy???\n","##imie_nazwisko_sXXXXX_WUM_ZAD7.ipynb\n","##To categorical"],"metadata":{"id":"XwoAwXtrMYVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unS3YPozcDCY"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd"]},{"cell_type":"code","source":["from tensorflow.keras.datasets import reuters"],"metadata":{"id":"mQvtfo2BcqIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nwords = 5000\n","(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = nwords)"],"metadata":{"id":"rhykc_bVcskv"},"execution_count":null,"outputs":[]}]}